---
title: Jotto - 5 words, 25 characters, 1500ms
description: Using approximations, rounding, and randomisation to solve the Jotto problem in under 1.5s
date: 2022-08-20
tags: [Optimization]
---

## Introduction

In short, given a collection of 5-character words, can we find 5 such that we cover 25 characters?

I was introduced to this problem by Matt Parker / Standup Maths through [this](https://www.youtube.com/watch?v=_-AfhLQfb6w) video, and coincidentally, I recently had an exam on approximation algorithms, so I thought, why not. Turns out, using an approximate solution, and then rounding can give us an optimal solution faster than - as far as I can tell - any other approach. Turns out, we can further improve our speed by introducing some randomisation.

## Abstracting away

The following is a common approach to solving optimization problems via approximations

1. Reduce the problem in polynomial time to an integer program
2. Relax the integer program to a linear program
3. Solve the linear program
4. Round the solution

### Why approximations?

Simply put, speed. When the task that we are trying to solve is [NP-Complete](https://en.wikipedia.org/wiki/NP-completeness), we can choose an algorithm that is

- Fast and correct but not general. The algorithm is correct and fast for a subset of inputs (e.g. Trees instead of arbitrary graphs).
- Correct and general but slow. The algorithm works for all classes of inputs, but takes exponential time at best.
- General and fast, but inaccurate. The algorithm solves general problems quickly but may be inaccurate and we can only prove correctness up to a bound.

In this case, we will go with the last approach.

### A first integer program

An integer program consists of an optimisation objective defined in terms of decision variables, and a set of constraints. Since we are using linear programming to build an approximate solution, we have to use a linear objective function, and linear constraints.

Our goal is to choose which words to keep such that all letters are covered. Thus, the natural integer program that comes to mind has the decision variables indicate whether the corresponding word is in the solution or not, and for each letter we have a constraint requiring the sum of the decision variables to be at least 1.

Putting it all together, we have

$$
\begin{align}
& \min\ c^T x_i \\\\
& \text{ subject to} \\\\
&\sum_{i: j \in i} x_i = 1 &\text{for all letters j} \\\\
& x_{i} \in \left\\{0, 1\right\\} & \text{for all i} \\\\
& c^T = \mathbb{1}
\end{align}
$$
Except that this isn't exactly correct, this program will construct solutions that cover all 26 letters. We will address this in a moment.

### A Linear relaxation and a rounding method

Linear programs require two things, a __linear__ objective function, and a set of __linear__ inequalities that define the constraints.
The handwavey explanation to why we require linear objective functions and constraints relates to convexity. Simply put, each linear inequality defines a halfspace where the variables are can be satisfied, and the intersection of said halfspaces is __always__ convex. In a convex region \\(R\\), any \\(x,y\in R, a\in [0,1]\\) satisfy \\(ax + (1-a)y \in R \\).

Linear programs are not as restrictive as integer programs when it comes to the decision variables as they allow for real values instead of integer ones.
Notice however that since linear programs can take integer solutions any solution to the integer program is a feasible solution,
that is, it is an acceptable solution to the linear program. This means that in the case of minimization,
the linear program can achieve solutions at least as good as those of the integer program; thus $$LP^\star \leq ZP^\star$$
where \\(LP^\star\\) is the value of the optimal solution to the linear program and \\(ZP^\star\\) is the value of the optimal solution to the integer program.

In general, we define linear programs to take positive values only, and we provide an upper bound on them. By restricting the values from above, we ensure that regardless of the coefficients in the objective function, 

#### Variable constraints

The minimization objective remains the same, we just need to ensure that the constraints are. First, we will turn the integer variables to "real" variables by requiring that
$$ x_{i} \geq 0 \text{ for all i}$$ Notice we are minimizing over a sum of positive numbers with positive coefficients, this means that unless there is advantage in getting values higher than 1, we do not need to provide an upper bound. If we wanted to provide one, we would do $$x_{i} \leq 1 \text{ for all i}$$.

#### Character constraints

Similarly to above, $$\sum_{i: j \in i} x_i = 1 \text{ for all letters j}$$, however, we do not need to use equality. Again, since this is a minimization objective, we can simply ask to have a value above or equal to 1. Since there is no advantage to having higher values, but instead there is a penalty, the LP will try to avoid higher values in the respective decision variables.

#### Rounding

We now require a rounding mechanism to go from the linear solution to the integer one. Given the decision variables \\(x\\), we select the variables that have a value \\(x_{i} \geq f\\), where \\[f = (max_{l} |\\{w| l\in w, \text{ w is a word} \\}|)^{-1} \\] where \\(l\\) is a letter of the alphabet. In essence, we round up the decision variables which have a value higher than the reciprocal of the maximum number of ways a given letter can be covered. This is guaranteed to give us a feasible but not necessarily optimal solution to the set cover problem. Can you prove why?

#### A problem with convexity

Because the intersection of halfspaces is convex, there can be eaxctly 0, 1 or infinite solutions. Zero solutions occur when the intersection of the halfspaces is empty.
If the intersection of the halfspaces is non empty, then it is a polyhedron. Due to the linear nature of the objective function, we can identify a direction which it increases or decreases. If we follow this direction and obey the constrainst, we will either terminate on a vertex, or on a plane. If we terminate on a vertex, then there exists a single solution, otherwise, there exist an infinite number. This can cause issues when it comes to rounding, so we will intead ask the solver to provide integer solutions.

## Back to Jotto

Notice that what we solved above is merely another instance of the set-cover problem, where the set we had to cover was the set of all letters in the alphabet. The jotto problem however does not ask for that, it simply asks to cover any combination of 25 letters. When we reduce a task to a decision problem, the decision problem simply tells us whether a solution given the parameters exists, thus if we would like to decide the minimum number to solve a task, we start from 0 and go up to find the minimum value where the decision problem returns True.

We will take a note from this approach and instead of trying to solve the full Jotto problem with LP, we will solve 26 set-cover problems, and stop as soon as we find a satisfying solution. For each letter, we will attempt to cover the remaining 25 letters using only words that don't have it.


## Other tricks
### Randomizing the runtime

Assuming that we do not know *which* letter we need to exclude, going over 26 characters means if the letter that will yield a solution is towards the end, we end up wasting precious time. We can take a hint from randomized algorithms and construct a solution that is faster on average / in the expectation simply by shuffling the order with which we go through the letters. If the target letter is after the middle, then on average we have a faster solution (hint, it is).

### Permutation invariant hashing

Assume for a moment that we have the words "abc", and "bca", the two words are simply a permutation of each other, thus we can get away with removing either of them without risking breaking the solution since the two words are equivalent. We can perform this grouping by using a permutation invariant hashing.

The simplest one in python is \\( hash \circ tuple \circ sorted \\), ie sort the letters, create a tuple and hash it.

Alternatively, one can create a hash value using the shift operator

```python
def invariant_shift(string):
    h = 0
    for c in string:
        h |= 1 << ord(c)
    return h
```

However this creates collisions with words like "aaabb" and "bbbaa" but it probably doesn't matter.

## Putting it all together

We define a parametrized linear program \\(LP(l)\\):

$$
\begin{align}
& min \sum_{i} x_i \\\\
& \text{ subject to} \\\\
&\sum_{i: j \in i} x_i \geq 1 &\text{for all letters j} \neq l \\\\
& x_{i} \geq 0 & \text{for each word i, where } l \not \in i
\end{align}
$$

### Imports and basic filtering

```python
from itertools import groupby
from toolz import compose
import numpy as np
from scipy.optimize import linprog
from random import shuffle

perm_invariant_hash = compose(hash, tuple, sorted)

words = open("/Users/quinn/Desktop/words_alpha.txt").readlines()
words = (word.strip().lower() for word in words)
words = (word for word in words if len(word) == 5)
words = (w for w in words if len(set(w)) == 5)
words = groupby(words, key=perm_invariant_hash)
words = [next(w) for (_, w) in words]

```

### Main Loop

```python
letters = list("abcdefghijklnopqrstuvwxyzm")
shuffle(letters)
for letter in letters:
    sol, rounded_solution = solve(letter, words)
    if sol is None:
        continue
    val = sum(sol.x)
    if len(rounded_solution) == 5:
        print(letter, rounded_solution)
        exit(0)
```

### LP

```python
def solve(letter, words:dict):
    words = [w for w in words if letter not in w]
    
    # since we are removing a letter, using ord(c) breaks
    letter_to_index = enumerate(l for l in "abcdefghijklmnopqrstuvwxyz" if l != letter)
    letter_to_index = {l: i for i, l in letter_to_index}
    n = len(words)

    # these are the coefficients to the linear program
    # since we are not doing weighted sum, using a vector of 1s gives us a simple summation    
    coefs = np.ones(n)

    # scipy's linprog needs upper bounds for the constraints, since the xs are positive,
    # we use negative coefficients with negative target values
    # for each letter, we create a row, where the ith column has -1 coefficient when 
    # the corresponding letter is in the word
    A_ub = np.zeros((25, n))

    for i, word in enumerate(words):
        for l in word:
            A_ub[letter_to_index[l], i] = -1

    b_ub = -np.ones(25)
    

    # we can make use of integrality parameter and ask the LP
    # give a solution where all variables are integers.
    # this makes the solution more costly, but avoids the
    # rounding errors due to infinite solutions.
    res = linprog(coefs, A_ub=A_ub, b_ub=b_ub, integrality=2*np.ones(n))

    if res.status != 0:
        return None, None    
    rounded_solution = [words[i] for i in range(len(words)) if res.x[i]]
    
    return res, rounded_solution
```

## Results

The solution my program finds is: `['bewig', 'fconv', 'hdqrs', 'japyx', 'klutz']`, which occurs when we remove `m`.
This takes 0.7s and 1.7s when m is evaluated first and last respectively on my baseline m1 macbook air.
The main cost is solving the LP/IP, thus optimizations that don't reduce the decision variables will barely affect the runtime.
